import requests
from bs4 import BeautifulSoup
from googlesearch import search
from sentence_transformers import SentenceTransformer
import chromadb
import logging
from urllib.parse import urlparse
from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer
import torch

# Setup logging
logging.basicConfig(level=logging.INFO)

# --- Step 1: Define Job Criteria & Google Dork Query ---
job_description = """
Looking for a Python Developer in India with strong skills in software development.
"""
google_query = 'site:linkedin.com/in/ "Python Developer" "India" -jobs -careers'

# --- Step 2: Search using Google Dorking and extract LinkedIn usernames ---
def extract_linkedin_usernames(query, num_results=5):
    usernames = []
    logging.info("Searching Google with query: %s", query)
    for url in search(query, num_results=num_results, lang="en"):
        parsed_url = urlparse(url)
        # Check if the URL contains '/in/'
        if "linkedin.com/in/" in url:
            # Split the path and filter out empty strings
            parts = [p for p in parsed_url.path.strip("/").split("/") if p]
            # We expect the first part to be "in" and the second part to be the username
            if parts and parts[0].lower() == "in" and len(parts) >= 2:
                username = parts[1]
                if username and username not in usernames:
                    usernames.append(username)
            else:
                logging.debug("URL did not have the expected structure: %s", url)
    return usernames

usernames = extract_linkedin_usernames(google_query, num_results=5)
logging.info("Extracted LinkedIn usernames: %s", usernames)

# --- Step 3: Retrieve LinkedIn profile details from RapidAPI ---
rapidapi_url = "https://linkedin-api8.p.rapidapi.com/"
rapidapi_headers = {
    "x-rapidapi-key": "YOUR_RAPIDAPI_KEY",  # Replace with your actual key
    "x-rapidapi-host": "linkedin-api8.p.rapidapi.com"
}

def get_linkedin_profile(username):
    querystring = {"username": username}
    logging.info("Fetching profile for username: %s", username)
    response = requests.get(rapidapi_url, headers=rapidapi_headers, params=querystring)
    if response.status_code == 200:
        return response.json()
    else:
        logging.warning("Failed to get profile for %s, status code: %s", username, response.status_code)
        return None

profiles_data = []
for username in usernames:
    profile_json = get_linkedin_profile(username)
    if profile_json:
        # Combine several fields to form a profile text; adjust keys based on API response
        profile_text = f"{profile_json.get('name', '')}\n{profile_json.get('headline', '')}\n{profile_json.get('summary', '')}"
        
        # Extract experience and skills if available
        experience = profile_json.get('experience', [])
        exp_text = ""
        for exp in experience:
            exp_text += f"\nRole: {exp.get('title', '')}\nCompany: {exp.get('company', '')}\nDuration: {exp.get('date_range', '')}\nDescription: {exp.get('description', '')}\n"
        
        skills = profile_json.get('skills', [])
        skills_text = "\nSkills: " + ", ".join(skills) if skills else ""
        
        full_profile = profile_text + exp_text + skills_text
        
        profiles_data.append({
            "username": username,
            "profile_text": full_profile,
            "source_url": f"https://www.linkedin.com/in/{username}"
        })

logging.info("Fetched profile details for %d users.", len(profiles_data))

# --- Step 4: Generate Embeddings for Each Profile ---
model = SentenceTransformer('all-MiniLM-L6-v2')
for profile in profiles_data:
    profile_text = profile["profile_text"]
    profile["embedding"] = model.encode(profile_text).tolist()

# --- Step 5: Setup ChromaDB for Storing Profile Embeddings ---
chroma_client = chromadb.Client()
collection_name = "linkedin_profiles"
if collection_name in [c.name for c in chroma_client.list_collections()]:
    collection = chroma_client.get_collection(name=collection_name)
else:
    collection = chroma_client.create_collection(name=collection_name)

# Insert profiles into the collection
for profile in profiles_data:
    collection.add(
        documents=[profile["profile_text"]],
        metadatas=[{"source_url": profile["source_url"], "username": profile["username"]}],
        ids=[f"profile_{hash(profile['username'])}"],
        embeddings=[profile["embedding"]]
    )

logging.info("Profiles stored in ChromaDB successfully.")

# --- Step 6: Implement RAG with LLM for Candidate Analysis ---

def load_llm_model():
    """Load LLM model for candidate analysis"""
    model_name = "mistralai/Mistral-7B-Instruct-v0.2"  # You can replace with a smaller model if needed
    
    # For lighter usage, you can use a pipeline instead
    logging.info(f"Loading LLM model: {model_name}")
    
    try:
        # Option 1: Full model loading (requires more RAM)
        # tokenizer = AutoTokenizer.from_pretrained(model_name)
        # model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map="auto")
        # return model, tokenizer
        
        # Option 2: Using pipeline (simpler)
        generator = pipeline('text-generation', model=model_name)
        return generator
    except Exception as e:
        logging.error(f"Error loading model: {e}")
        # Fallback to a smaller model if the first one fails
        generator = pipeline('text-generation', model="distilgpt2")
        return generator

def analyze_candidates(job_description, top_n=3):
    """
    RAG implementation to analyze and rank candidates based on job description
    """
    # Embed the job description
    job_embedding = model.encode(job_description).tolist()
    
    # Retrieve relevant profiles from ChromaDB
    query_result = collection.query(
        query_embeddings=[job_embedding],
        n_results=top_n,
    )
    
    if not query_result['documents'] or not query_result['documents'][0]:
        logging.warning("No matching profiles found")
        return []
    
    # Load LLM for analysis
    llm = load_llm_model()
    
    analyzed_candidates = []
    
    # Analyze each candidate with the LLM
    for i, profile_text in enumerate(query_result['documents'][0]):
        username = query_result['metadatas'][0][i]['username']
        
        # Construct prompt for the LLM
        prompt = f"""
        Task: Evaluate a candidate's LinkedIn profile against a job description.
        
        Job Description:
        {job_description}
        
        Candidate Profile:
        {profile_text}
        
        Please analyze this candidate and provide:
        1. Match Score (0-100%)
        2. Strengths relative to the job
        3. Gaps or mismatches
        4. Overall recommendation (Highly Recommended, Recommended, Consider, Not Recommended)
        """
        
        # Generate analysis with LLM
        try:
            if isinstance(llm, pipeline):
                response = llm(prompt, max_length=1000, num_return_sequences=1)
                analysis = response[0]['generated_text']
            else:
                # If using the full model approach
                inputs = llm[1](prompt, return_tensors="pt").to(llm[0].device)
                outputs = llm[0].generate(**inputs, max_length=1000)
                analysis = llm[1].decode(outputs[0], skip_special_tokens=True)
            
            # Store the analysis
            analyzed_candidates.append({
                "username": username,
                "profile_url": f"https://www.linkedin.com/in/{username}",
                "analysis": analysis
            })
            
        except Exception as e:
            logging.error(f"Error analyzing candidate {username}: {e}")
    
    return analyzed_candidates

# --- Step 7: Run the RAG analysis on the candidates ---
def main():
    logging.info("Starting candidate analysis...")
    results = analyze_candidates(job_description)
    
    print("\n===== CANDIDATE ANALYSIS RESULTS =====\n")
    for i, result in enumerate(results, 1):
        print(f"Candidate {i}: {result['username']}")
        print(f"Profile: {result['profile_url']}")
        print(f"\nAnalysis:\n{result['analysis']}")
        print("\n" + "-"*50 + "\n")
    
    if not results:
        print("No candidates found matching the criteria.")

if __name__ == "__main__":
    main()